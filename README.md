# OLMo_thesis
This repository contains the codebase for the masterâ€™s thesis Temporal Adaptation Techniques in Diachronic Language Modelling.
It implements several fine-tuning and adaptation strategies for historical language modelling, including a Mixture of Experts (MoE) architecture trained on temporally split English corpora. Key components include temporal conditioning, soft-gated expert selection, and a continuous year prediction heuristic based on gating probabilities. Experiments are conducted on the CLMET corpus, with model training based on OLMo and Hugging Face libraries.
